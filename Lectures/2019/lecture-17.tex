% !TEX spellcheck = en_US
% !TEX spellcheck = LaTeX
\documentclass[letterpaper,english,10pt]{article}
\input{../../header}

\title{Lecture-17: Random Energy Model}


\begin{document}
\maketitle



There are a number of real-life systems for which disorder is frozen in, or quenched, as a result of the material preparation. 
A typical example is an alloy with substitution disorder. 
Although in that case we would be interested in a single realization of that disorder (a real material), it may still be useful to consider the average over all possible realizations of such disorder. 
The properties of the different realizations are indeed typically equivalent in the thermodynamic limit. 
We discuss these concepts below in more details.

\section{Definition}
The simplest disordered model is the Random Energy Model (REM), which was introduced by Derrida in 1980. 
The density function of the energy $E_i$ for state $i$ is given by 
\EQ{
P(E)=\frac{1}{\sqrt{\pi N}}e^{-E^2/N}.
}
The position of its $M = 2^N$ energy levels is quenched, i.e., frozen or fixed. 
That is, $(E_1, \dots, E_{2^N})$ is a set of $2^N$ energy levels each chosen independent and identically from the distribution $P(E)$, 
and the occupancy of state $i$ is given by the Boltzmann probability
\EQ{
\mu_\beta(j)= \frac{1}{Z_N(\beta)}e^{-\beta E_j}, Z_N(\beta) = \sum_{j=1}^{2^N}e^{- \beta E_j}.
}
Studying the REM thus consists of examining a probabilistic object over levels that are themselves probabilistically distributed. 
The REM is a \textbf{disordered model}: the energy is not a deterministic function, but rather a stochastic process. 
A particular realization of such a process is usually called a \textbf{sample} (or \textbf{instance}). 
Each realization of disorder is a sample, but calculating the properties of a specific sample with finite $N$ is typically very difficult--it corresponds to finding an optimal distribution of a particular instance of a problem. We will instead average over these samples, which we denote $\E(\cdot)$, and is typically a much easier calculation to perform. 
Because a lot of systems have thermodynamic potentials $X$ that self-average, that is, ``concentrate'' as $N$ increases, i.e., for any tolerance $\theta > 0$
\EQ{
\lim_{N\to\infty}\Pr\set{\abs{\frac{X_N}{N}-\frac{\E X_N}{N}} \ge \theta} = 0,
}
this calculation is also meaningful for large systems. Because the concentration is also often exponential in $N$, 
\EQ{
\lim_{N\to\infty}\Pr\set{\abs{\frac{X_N}{N}-\frac{\E X_N}{N}} \ge \theta} \overset{\cdot}{=} e^{-N\delta_X},
}
where $\delta_X$ is a positive constant that may depend on the thermodynamic potential. 
Because this is the case for the REM, $\E X_N$ is often a good estimate, 
even for systems with finite $N$. 

\section{REM thermodynamics}
Let us now calculate the partition function of the disorder-averaged REM. 

\subsection{Entropy}
Consider an energy interval $\cI = [N\epsilon, N (\epsilon + \delta)]$, 
and the number of energy levels within that interval $\cN (\epsilon, \epsilon + \delta)$. 
For each state $i$, the probability that $E_i \in \cI$ is independent from other states is given by 
\EQ{
P_\cI  = \sqrt{\frac{\pi}{N}}\int_{\epsilon}^{\epsilon+\delta}e^{-Nx^2}dx,
}
by changing the variable $x = E/N$, integrated with $dx = dE/N$ for the variable $x \in [\epsilon, \epsilon+\delta]$. 
Therefore, it follows that $\cN (\epsilon, \epsilon + \delta) = \sum_{i=1}^{2^N}\indicator{E_i \in \cI}$ is binomially distributed, as sum of $2^N$ \emph{i.i.d.} Bernoulli random variables with mean $P_\cI$. 
It follows that 
\meq{3}{
&\E\cN(\epsilon, \epsilon + \delta)  = 2^N P_\cI,&
&\Var{\cN(\epsilon, \epsilon + \delta)} = 2^NP_\cI(1-P_\cI),&
&\frac{\Var{\cN (\epsilon, \epsilon + \delta)}}{(\E\cN(\epsilon, \epsilon + \delta))^2} = 2^{-N}\left(\frac{1}{P_\cI}-1\right) \approx \frac{1}{2^NP_\cI}.
}
Defining $s_a(x) \triangleq \ln 2 - x^2$, we observe that 
\meq{2}{
&\E\cN(\epsilon, \epsilon + \delta)  \stackrel{\cdot}{=} \exp\left[N\max_{x \in [\epsilon, \epsilon+\delta]}s_a(x)\right],&
&\frac{\Var{\cN (\epsilon, \epsilon + \delta)}}{(\E\cN(\epsilon, \epsilon + \delta))^2} \stackrel{\cdot}{=} \exp\left[-N\max_{x \in [\epsilon, \epsilon+\delta]}s_a(x)\right].
}
%by taking the saddle point approximation. 
Note also that $s_a(x) \ge 0$ iff $x \in [-\epsilon^\ast, \epsilon^\ast]$ where $\epsilon^\ast = \sqrt{\ln2}$.  

When $\epsilon \notin [-\epsilon^{\ast}, \epsilon^\ast]$, the average density of energy levels is exponentially small in $N$: for a typical sample, there is no configuration at energy $E_i \approx N\epsilon$. 
In contrast, when $\epsilon \in (-\epsilon^{\ast}, \epsilon^\ast)$, 
there is an exponentially large density of levels, and the fluctuations of this density are very small. 
This result is illustrated by a small numerical experiment in Fig. 5.1. 

\begin{prop}
Defining the entropy function 
\EQ{
s(\epsilon) = 
\begin{cases}
s_a(\epsilon), & \abs{\epsilon} \le \epsilon^\ast,\\
-\infty, & \abs{\epsilon} > \epsilon^\ast.
\end{cases}
}
then, for any pair $\epsilon$ and $\delta$, with probability one 
$\lim_{N\to\infty}\cN(\epsilon, \epsilon+\delta) = \sup_{x \in [\epsilon, \epsilon+\delta]}s(x)$.
\end{prop}
\begin{proof}
Assume an interval $[\epsilon, \epsilon+\delta]$ disjoint from $[-\epsilon^\ast, \epsilon^\ast]$, 
and 
\EQ{
\E\cN(\epsilon, \epsilon+\delta) \stackrel{\cdot}{=}e^{-AN},
}
where $A = \sup_{x \in [\epsilon, \epsilon+\delta]}s_a(x) > 0$. 
Since $\cN$ is an integer, we have from Markov inequality 
\EQ{
P\set{\cN(\epsilon, \epsilon+\delta)> 0} \le \E\cN(\epsilon, \epsilon+\delta) \le e^{-AN}.
}
That is, the probability of having an energy level in a fixed interval outside of $[-\epsilon^\ast, \epsilon^\ast]$ is exponentially small in $N$.
\end{proof}

\subsection{Phase transitions}
Looking at the thermodynamic limit $\lim_{N \to \infty}$and then taking $\lim_{\delta \to 0}$, 
we get to exponential order that
\EQ{
Z_N(\beta) \stackrel{\cdot}{=} \int_{-\epsilon^\ast}^{\epsilon^\ast}e^{N(s_a(\epsilon)-\beta\epsilon)}d\epsilon.
}
By analogy with our discussion in Chapter 2 about the concentration around the saddle
point in the thermodynamic limit, (but noting that there is an additional difficulty because the energy levels are now themselves fluctuating), we get
\EQ{
\phi(\beta)=\lim_{N\to\infty}\frac{1}{N}\ln Z_N(\beta) = \max_{\epsilon \in [-\epsilon^\ast, \epsilon^\ast]} [s_a(\epsilon) -\beta\epsilon] = 
\begin{cases}
\frac{\beta^2}{4}+\ln 2, & \beta \le \beta_c\\
\beta\sqrt{\ln 2}, & \beta > \beta_c,
\end{cases}
}
for $\beta_c = 2\sqrt{\ln2}$. 
This follows from the fact that $s_a(x)-\beta x = \ln2-x^2-\beta x = \ln 2 +\frac{\beta^2}{4}-\left(x+\frac{\beta}{2}\right)^2$. 
The right hand side is maximized for $\beta = -2x \in [0, 2\epsilon^\ast]$, or $\beta \le \beta_c = 2\sqrt{\ln2}$. 
When $\beta > \beta_c$, we can't make the square term zero, and the right hand side is maximized by minimizing the square term for $x = -\epsilon^\ast = -\sqrt{\ln2}$ which implies $s_a(-\sqrt{\ln2})+\beta\sqrt{\ln2} = \beta\sqrt{\ln2}$. 

This point is a second-order phase transition, which in the language of disordered systems is also often called a `random first-order' transition (we will come back to the meaning of this label later). 
This geometrical construction can be seen in Fig. 2.

\subsubsection{Properties of two phases}
\begin{itemize}
\item In the high-temperature phase, i.e., $\beta \le \beta_c$, the internal energy $u(\beta) = -\pd{\phi(\beta)}{\beta} = -\frac{\beta}{2}$ and the canonical entropy $s(\beta) = \beta u(\beta) +\phi(\beta) = \ln2- \frac{\beta^2}{4}$, 
hence the Boltzmann measure is dominated by configurations with an energy $E_i \approx -\frac{N\beta}{2}$, 
and the model tends to random spins as $\beta \to 0$. 

\item In the low-temperature phase, i.e., $\beta > \beta_c$, the internal energy $u(\beta) = -\sqrt{\ln2} = -\epsilon^\ast$ and $s(\beta) = 0$, hence the Boltzmann measure is dominated by a set of levels of constant energy, whose number grows sub-exponentially with $N$. 
\end{itemize}
\subsection{Condensation Phenomenon}
In order to measure the degree of concentration of the measure, we define the \textbf{participation ratio function}
\EQ{
Y_N(\beta) \triangleq \sum_{j=1}^{2^N}\mu_\beta(j) = \frac{\sum_{j=1}^{2^N}e^{-2\beta E_j}}{\left(\sum_{j=1}^{2^N}e^{-\beta E_j}\right)^2} = \frac{Z_N(2\beta)}{Z_N(\beta)^2}. 
}
In general, in the high temperature limit $\beta \to 0$, the participation is distributed evenly over all the $2^N$ levels,
hence we get $Y_N (0) = 2^{-N}$. 
If we take the low-temperature setup used in Chapter 2, we would get instead $\lim_{\beta \to \infty}Y_N(\beta) = \frac{1}{\abs{\sX_0}}$. 
In other words, very few states meaningfully contribute to the partition function.
\begin{itemize}
\item For the high-temperature phase of the REM, i.e., for $\beta \le \beta_c = 2\sqrt{\ln2}$, we have
\EQ{
\E Y_N(\beta)\indicator{\beta \le \beta_c} \stackrel{\cdot}{=} \frac{e^{N\phi(2\beta)}}{e^{2N\phi(\beta)}} = \frac{e^{N(\ln2 + \beta^2)}}{e^{2N(\ln 2+ \frac{\beta^2}{4})}} = e^{-N(\ln2 - \frac{\beta^2}{2})}, 
}
which decays exponentially with $N$ for $\beta $. 
The measure is therefore broadly distributed at high temperatures, i.e., $\lim_{N \to \infty}\E Y_N(\beta)\indicator{\beta  < \frac{\beta_c}{\sqrt{2}}} = 0$. 
 
 \item At low temperatures, a naive calculation gives
 \EQ{
 \E Y_N(\beta)\indicator{\beta > \beta_c} = 1,
 }
which is inaccurate -- although a single (ground) state is indeed in control when $\beta \to \infty$. 
The problem is due to the fact that the sub-exponential terms in $N$ should be taken into account in the calculation. 
Because they have been neglected in the calculation of $Z_N(\beta)$, they do not appear here. 
In reality, $Y_N (\beta)$ is finite and fluctuates a lot from sample to sample (as suggested by the above calculation). 
It is shown in Appendix~\ref{appendix:Condensation} that in the thermodynamic limit, we instead get
\EQ{
\E Y(\beta) = \begin{cases}
0, & \beta < \beta_c\\
1- \frac{\beta_c}{\beta}, & \beta \ge \beta_c.
\end{cases}
}
Hence, we see that condensation begins at $\beta_c$, and that the measure becomes increasingly concentrated until the ground state dominates, as $T \to 0$.  
\end{itemize}

\subsection{Annealed vs. Quenched}
When the free energy density is self-averaging, the value of $\phi_N$ is roughly the same for all samples and can be calculated through the quenched average
\EQ{
\phi_{N,q} \triangleq \frac{1}{N}\E\ln Z_N, 
}
but in general it can be quite difficult to compute this quantity. 
The annealed average
\EQ{
\phi_{N,a} \triangleq \frac{1}{N}\ln \E Z_N, 
}
allows the energy levels themselves to thermalize and gives here
\EQ{
\E Z_N =\E\sum_{i=1}^{2^N}e^{-\beta E_i} =  2^N\E e^{-\beta E_i} = 2^N\frac{1}{\sqrt{\pi N}}\int_{E \in \R}e^{-\beta E-\frac{E^2}{N}} = e^{N(\ln2+\frac{\beta^2}{4})},
}
which means that $\phi_{N,a} = \frac{\beta^2}{4}+\ln2$. 
By chance, for this model the result is the same as the high-temperature result, but it completely misses the phase transition and gives a negative entropy $s(\beta) = \ln2-\frac{\beta^2}{4}$ for temperatures below $\frac{1}{\beta_c}$.  
This negative entropy is a clear signal of failure of the calculation. 
The reason for the failure is as follows. 
For a given sample with a free energy density $f_N (\beta)$, the partition function behaves as 
\EQ{
Z_N = \exp[-\beta N f_N(\beta)].
}
Self-averaging means that $f_N(\beta)$ has small sample-to-sample fluctuations. 
These fluctuations, however, do exist and their impact is amplified by the factor of $N$ in the exponent. The partition function can thus be dominated by some very rare samples, i.e., those with an anomalously low value of $f_N(\beta)$. 

Consider, for instance, the low-temperature limit. 
We know that in almost all samples the configuration with the lower energy density is found at $E_i \approx N\epsilon^\ast$. 
There exist, however, exceptional samples, where one configuration has an energy smaller $E_i = -N\epsilon$ with $\epsilon > \epsilon^\ast$. 
Because these samples are exponentially rare (with probability $\stackrel{\cdot}{=}2^Ne^{-N\epsilon^2}$, they are irrelevant for the quenched average, but they dominate the annealed average.
%\begin{appendices}
\appendix
\section{REM Condensation}
\label{appendix:Condensation}
Using the identity 
\EQ{
\frac{1}{Z^2} = \int_{0}^{\infty}te^{-tZ}dt, 
}
we get $for M = 2^N$
\eq{
\E Y_N(\beta) &=\E \frac{Z_N(2\beta)}{Z_N(\beta)^2} = \E \int_{0}^\infty Z_N(\beta)e^{-tZ_N(\beta)}dt\\
&= \E\int_{0}^\infty \left(\sum_{j=1}^Me^{-2\beta E_j}\right) t e^{-t\sum_{j=1}^Me^{-\beta E_j}} dt 
= M \E\int_{0}^\infty t e^{-2\beta E_1-t\sum_{j=1}^Me^{-\beta E_j}} dt\\
&= M \E\int_{0}^\infty t e^{-2\beta E_1-te^{-\beta E_1} -\sum_{j=2}^Me^{-\beta E_j}} dt = M\int_0^\infty ta(t)[1-b(t)]^{M-1}dt, 
}
where we have separated the averaging over $E_1$ and $E_{i \neq1}$ as 
\meq{2}{
&a(t)\triangleq \E e^{-2\beta E-te^{-\beta E}}, &&b(t)\triangleq \E\left(1- e^{-te^{-\beta E}}\right). 
}
In order to get information about the scaling of $P(E)$ at low temperatures, 
we need to expand around the function around the ground state energy. 
The strategy below is to identify its first sub-extensive correction. 
We thus consider a regime where for a system of $2^N$ states the probability of having a state of energy $-N\epsilon_0$ is high, hence we get 
\EQ{
2^NP_N(-N\epsilon_0) = \frac{ 2^N}{\sqrt{\pi N}}e^{-N\epsilon^2} \approx 1, 
}
which corresponds to an estimate of the ground state in a finite $N$ system. 
Taking the log on both sides and solving for $\epsilon_0$ gives in the large $N$ limit 
\EQ{
\epsilon_0  = \sqrt{\ln2 - \frac{1}{N}\ln\sqrt{\pi N}} 
= \sqrt{\ln2} - \frac{\ln\sqrt{\pi N}}{N2\sqrt{\ln2}}  + O(N^{-2}) 
\approx \epsilon^\ast - \frac{1}{2\epsilon^\ast}\frac{\ln\sqrt{\pi N}}{N},
}
 which includes the asymptotic correction to the ground state energy $\epsilon^\ast = \sqrt{\ln 2}$. 
 %Note the missing N in Eq. (5.22) ? see book errata.
Because we expect the energy in the low-temperature phase to be concentrated around its ground
state $\epsilon_0$, we make an expansion around it by writing 
\meq{2}{
&E = -N\epsilon + u, &&t = ze^{-N\beta\epsilon_0}.
}
We then get that in the large $N$ limit 
\EQ{
\ln P(E)=-\ln\sqrt{\pi N} - \frac{(N\epsilon_0+u)^2}{N}
=-N\epsilon_\ast^2+2\epsilon_\ast u + \Theta(1/N) \approx \ln\left(2^{-N}e^{\beta_cu}\right),
}
where $\beta_c = 2\sqrt{\ln2}$, and hence we get that in the low energy limit 
\EQ{
P(E) \approx \frac{1}{M}e^{\beta_cu}.
}
We thus obtain (note that large $u$ corrections quickly vanish) 
\eq{
a(t) &\simeq \frac{1}{M}e^{2N\beta\epsilon_0}\int_{-\infty}^\infty du e^{\beta_cu-2\beta u-ze^{-\beta u}} 
= \frac{e^{2N\beta\epsilon_0}}{M\beta}z^{\frac{\beta_c}{\beta-2}}\Gamma(2-\frac{\beta_c}{\beta})\\
b(t) &\simeq \frac{1}{M}\int_{-\infty}^{\infty}du e^{-\beta_cu}\left(1 -  e^{-ze^{-\beta u}}\right) 
= -\frac{1}{M\beta}z^{\frac{\beta_c}{\beta}}\Gamma\left(-\frac{\beta_c}{\beta}\right).
}
Noting that $[1 - b(t)]^{M-1} \stackrel{\cdot}{=}  e^{-Mb(t)}$ and after doing the proper change of variables, one obtains in the low temperature limit
\EQ{
\E Y(\beta) = \frac{1}{\beta}\Gamma(2-\frac{\beta_c}{\beta})\int_0^\infty z^{\frac{\beta_c}{\beta}-1}e^{\frac{1}{\beta}\Gamma(-\frac{\beta_c}{\beta})z^{\frac{\beta_c}{\beta}}} + \Theta(N^{-1}) = 1 - \frac{\beta_c}{\beta}, 
}
in the large $N$ limit and low temperature limit. 
%\end{appendices}
\end{document}
