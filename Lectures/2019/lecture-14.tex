
% !TEX spellcheck = en_US
% !TEX spellcheck = LaTeX
\documentclass[letterpaper,english,10pt]{article}
\input{../../header}
\usepackage{bm}
\graphicspath{{./Figures/}}

\title{Lecture-14: Markov Chain Mixing}


\begin{document}
\maketitle

\section{Comparison of Monte Carlo Methods}
We have seen that Monte Carlo sampling methods give us the desired stationary distribution starting from a base Markov chain. 
Assuming one step of the Markov chain takes unit time, 
we can compare their computational efficiency in terms of number of operations needed per sample,  
and the number of sampled needed to reach \textit{close enough} to the desired stationary distribution. 

\subsection{Total variation distance}
The closeness of two distributions can be measured by the following distance metric.
\begin{defn}
The \textbf{total variation distance} between two probability distributions $\mu$ and $\nu$ on $\cX^N$ is defined by
\EQ{
\norm{\mu-\nu}_{\text{TV}} = \max\set{\abs{\mu(A) - \nu(A)}: A \subseteq \sX^N}.
}
\end{defn}
This definition is probabilistic in the sense that the distance between $\mu$ and $\nu$ is the maximum difference between the probabilities assigned to a single event by the two distributions. 
\begin{prop}
Let $\mu$ and $\nu$ be two probability distributions on the configuration space $\sX^N$. 
Let $B = \set{x \in \sX^N: \mu(x) \ge \nu(x)}$, then 
\EQ{
\norm{\mu-\nu}_{\text{TV}} = \frac{1}{2}\sum_{x \in \sX^N}\abs{\mu(x)-\nu(x)} = \sum_{x \in B}[\mu(x)-\nu(x)].
}
\end{prop}
\begin{proof}
Let $A \subseteq \sX^N$ be any event. 
Since $\mu(x)  - \nu(x) < 0$ for any $x \in A\cap B^c$, we have 
\EQ{
\mu(A) - \nu(A) \le \mu(A \cap B) - \nu(A \cap B) \le \mu(B) - \nu(B). 
}
Similarly, we can show that 
\EQ{
\nu(A) - \mu(A) \le \nu(B^c) - \mu(B^c) = \mu(B) - \nu(B).
}
It follows that $\abs{\mu(A)-\nu(A)} \le  \mu(B) - \nu(B)$ for all events $A$, 
and the equality is achieved for $A = B$ and $A = B^c$. 
Thus, we get that 
\EQ{
\norm{\mu-\nu}_{\text{TV}} = \frac{1}{2}[\mu(B)-\nu(B) + \nu(B^c)-\mu(B^c)] = \frac{1}{2}\sum_{x \in \sX^N}\abs{\mu(x)-\nu(x)}.
}
\end{proof}
\begin{exerc}
Show that $\norm{\mu-\nu}_{\text{TV}}$ is a distance.  
\end{exerc}
\begin{prop}
Let $\mu$ and $\nu$ be two probability distributions on the configuration space $\sX^N$. 
For any observable $f: \sX^N \to \R$, we have 
\EQ{
\norm{\mu-\nu}_{\text{TV}} = \frac{1}{2}\sup\set{\sum_{x \in \sX^N}f(x)\mu(x) - \sum_{x \in \sX^N}f(x)\nu(x): \max_{x \in \sX^N}\abs{f(x)} \le 1}.
}
\end{prop}
\begin{proof}
If $\max_x\abs{f(x)} \le 1$, then it follows that 
\EQ{
\frac{1}{2}\abs{\sum_xf(x)(\mu(x)-\nu(x)} \le \frac{1}{2}\sum_x\abs{\mu(x)-\nu(x)} = \norm{\mu-\nu}_{\text{TV}}.
}
For the reverse inequality, we define $f^{\ast}(x) = \indicator{x \in B}-\indicator{x \notin B}$ in terms of the set $B = \set{x \in \sX^N: \mu(x) \ge \nu(x)}$. 
It is clear that $\max_x\abs{f(x)} = 1$, and we have 
\EQ{ 
\frac{1}{2}\abs{\sum_xf^{\ast}(x)(\mu(x)-\nu(x)} = \frac{1}{2}\sum_x\abs{\mu(x)-\nu(x)} = \norm{\mu-\nu}_{\text{TV}}.
}
\end{proof}
\subsection{Coupling and total variation distance}
\begin{defn}
A \textbf{coupling} of two probability distributions $\mu$ and $\nu$ is a pair of random variables $(X, Y)$ 
defined on a single probability space such that the marginal distribution of $X$ is $\mu$ and the marginal distribution of $Y$ is $\nu$. 
That is, a coupling $(X, Y)$ satisfies $P\set{X = x} = \mu(x)$ and $P\set{Y = y} = \nu(y)$. 
\end{defn}

Any two distributions $\mu$ and $\nu$ have an independent coupling. 
However, when the two distributions are not identical, 
it will not be possible for the random variables to always have the same value. 
Total variation distance between $\mu$ and $\nu$ determine how close can a coupling get to having $X$ and $Y$ identical. 
\begin{prop}
Let $\mu$ and $\nu$ be two distributions on the configuration space $\sX^N$, then 
\EQ{
 \norm{\mu-\nu}_{\text{TV}}  = \inf\set{P\set{X\neq Y}: (X,Y) \text{ a coupling of distributions } \mu, \nu}. 
}
\end{prop}
\begin{proof}
For any coupling $(X,Y)$ of the distributions $\mu, \nu$ and any event $A \subseteq \sX^N$, we have 
\EQ{
\mu(A)-\nu(A) = P\set{X \in A} - P\set{Y \in A} \le P\set{X \in A, Y \notin A} \le P\set{X \neq Y}.
}
Therefore, it follows that $\norm{\mu-\nu}_{\text{TV}}  \le P\set{X\neq Y}$ for all  couplings $(X,Y)$ of distributions $\mu, \nu$. 
\end{proof}
\begin{defn}
For two distributions $\mu$ and $\nu$ on the configuration space $\sX^N$, 
the coupling $(X,Y)$ is \textbf{optimal} if  
\EQ{
\norm{\mu-\nu}_{\text{TV}} = P\set{X \neq Y}.
}
\end{defn}

\end{document}
