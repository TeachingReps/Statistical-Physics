% !TEX spellcheck = en_US
% !TEX spellcheck = LaTeX
\documentclass[letterpaper,english,10pt]{article}
\input{../../header}

\title{Lecture-16: Mixing times}


\begin{document}
\maketitle

\section{Mixing times}
\begin{defn}
For an irreducible, aperiodic, positive recurrent homogeneous Markov chain $X$ with transition probability matrix $P$ and invariant distribution $\pi$, 
the \textbf{mixing time} is defined as 
\meq{2}{
&t_{\text{mix}}(\epsilon) \triangleq \min\set{t \ge 0: d(t) = \max_{x \in \sX^N}\norm{P^t(x, \cdot)- \pi(\cdot)}_{\text{TV}} \le \epsilon},&&\text{ and } t_{\text{mix}} \triangleq t_{\text{mix}}(1/4).
}
\end{defn}
We can write the following relation 
\EQ{
d(\ell t_{\text{mix}}(\epsilon)) \le \bar{d}(\ell t_{\text{mix}}(\epsilon)) \le \bar{d}( t_{\text{mix}}(\epsilon))^\ell \le 2^\ell d(t_{\text{mix}}(\epsilon))^\ell \le (2\epsilon)^\ell. 
}
For $\epsilon = 1/4$, since $d$ is a non-increasing function, %and $d(t_{\text{mix}}(\epsilon)) \le \epsilon$, 
we can write 
\meq{2}{
&d(\ell t_{\text{mix}}) \le 2^{-\ell} = \epsilon, && t_{\text{mix}}(\epsilon) \le \left\lceil\ln \frac{1}{\epsilon}\right\rceil t_{\text{mix}}.
}

Often one is interested in computing the mean of some observable $\cO: \sX^N \to \R$, 
rather than finding a configuration with the equilibrium distribution. 
Denoting the expectation with respect to the equilibrium distribution as $\inner{}$, 
we observe that 
\EQ{
\inner{\cO(X)} = \sum_{x \in \sX^N}\cO(x)\pi(x) = \lim_{t \to \infty}\frac{1}{t}\sum_{s=1}^t\cO(X_s), 
}
mean observable can be computed by averaging over configurations generated by the equilibrium distribution. 
Suppose $X_0 \sim \pi$, then how many steps are needed to reach close to $\E X_s$? 
In practice, we don't know $\pi$ and we can reach $\epsilon$ close to it by $P^t(x,\cdot)$ in $\left\lceil\ln \frac{1}{\epsilon}\right\rceil t_{\text{mix}}$ steps for any initial configuration $x$.  
Defining $\cO_t = \cO(X_t)$, we can estimate the mean of the observable by empirical estimate 
\EQ{
\bar{\cO}_t = \frac{1}{t}\sum_{s=0}^{t-1}\cO_s.
}
Since the joint distribution of $(X_s, X_u)$ is same as $(X_0, X_{u-s})$ for $u \ge s$ for a stationary Markov chain, 
we have $\inner{\cO_s; \cO_u} = \inner{\cO_0; \cO_{u-s}}$. 
We further define the \textbf{autocorrelation function} for the observable $\cO$ as $C_\cO(t-s) \triangleq \frac{\inner{\cO_t; \cO_s}}{\inner{\cO_0; \cO_0}}$. 
We can check that the mean of this estimate is $\inner{\bar{\cO}_t} = \inner{\cO(X)}$ and the variance is given by 
\EQ{
\Var{\bar\cO_t} = \frac{1}{t^2}\sum_{s,u=0}^{t-1}\inner{\cO_s; \cO_u} 
%= \frac{1}{t^2}\left[\sum_{u=0}^{t-1}\left(\sum_{s =0}^{u-1} + \sum_{s=u}+\sum_{s=u+1}^{t-1} \right)\inner{\cO_s; \cO_u}\right]\\
%&= \frac{1}{t^2}\left[\sum_{u=0}^{t-1}\sum_{s=0}^{u-1} \inner{\cO_0; \cO_{u-s}} +t\inner{\cO_0; \cO_0} + \sum_{u=0}^{t-1}\sum_{s=u+1}^{t-1}\inner{\cO_0; \cO_{s-u}} \right]\\
%&= \frac{1}{t^2}\left[\sum_{s=0}^{t-1}\sum_{u=1}^{t-s-1} \inner{\cO_0; \cO_{u}} +t\inner{\cO_0; \cO_0} + \sum_{u=0}^{t-1}\sum_{s=1}^{t-u-1}\inner{\cO_0; \cO_{s}} \right]\\
%&= \frac{1}{t^2}\left[\sum_{s=1}^{t-1}2(t-s) \inner{\cO_0; \cO_{u}} +t\inner{\cO_0; \cO_0}  \right]\\
%&= \frac{1}{t^2}\sum_{s=0}^{t-1}(t-s)\inner{\cO_0; \cO_s},
= \frac{\inner{\cO_0; \cO_0}}{t}\left[-\sum_{s=1}^{t-1}\frac{s}{t}C_\cO(s) +C_\cO(0) + \sum_{s=1}^{t-1}2C_\cO(s) \right].
}
 %so that $\Var{\bar\cO_t} = \frac{\inner{\cO; \cO}}{t^2}\sum_{s=0}^{t-1}(t-s)C_\cO(t)$. 
For irreducible, aperiodic, and positive recurrent Markov chains, the autocorrelation function $\cO(t)$ for certain observables, decreases exponentially as $t \to \infty$. 
Defining the \textbf{integrated autocorrelation time} $t^{\cO}_{\text{int}} \triangleq C_\cO(0) + \sum_{t=1}^\infty 2C_\cO(t)$, we can write 
\EQ{
\Var{\bar\cO_t} = \frac{t^\cO_{\text{int}}}{t}\Var{\cO_0} + O(t^{-2}). 
}
The integrated autocorrelation time $t^{\cO}_{\text{int}}$ determines how long the Monte Carlo simulation should run to get good estimate for the mean $\inner{\cO}$. 
\begin{shaded*}
\begin{exmp}[Curie-Weiss model]
\end{exmp}
\end{shaded*}
\section{Simulated annealing}
Suppose we are interested in solving an optimization problem in the configuration space $\sX^N$ with the cost function $E(x)$. 
Let $\min_xE(x) = E_0 = 0$, without loss of generality, 
then we are interested in finding the set $\sX^N_0 = \set{x \in \sX^N: E(x) = E_0 = 0}$. 
This problem can be posed as a statistical-physics problem, where the question is to find a Markov chain such that the equilibrium distribution is given by the Boltzmann distribution for energy function $E(x)$ and inverse temperature $\beta$. 
That is, 
\EQ{
\mu_\beta(x) = \frac{1}{Z(\beta)}e^{-\beta E(x)}, \text{ where } Z(\beta) = \sum_y\mu_\beta(y).
}

A Monte Carlo method provides a sampling method from any base Markov chain, 
such that the stationary distribution of the resulting Markov chain is the desired Boltzmann distribution. 
In this method, for any finite inverse temperature at stationarity, 
\EQ{
\mu_\beta(\sX_0^N) = \frac{\abs{\sX_0^N}}{Z(\beta)}.
}
From the ergodic theory, we know that mean time to visit an optimal configuration is given by $\frac{1}{\mu_\beta(\sX_0^N)}$. 

\section{Sanov's theorem}
Let $X \in \sX^N$ be a random vector with $N$ \emph{i.i.d.} $\sX$ valued random variables $X_1, \dots, X_N$ with the common underlying distribution $p \in \cM(\sX)$.  
We denote the type of a sequence by $\cT : \sX^N \to \cM(\sX)$ and write
\EQ{
\cT(X) = \frac{1}{N}\sum_{i=1}^N\indicator{X_i = x}. 
}
We can compute the probability of the type of a random sequence $X$ to be a distribution $q \in \cM(\sX)$ as 
\EQ{
P\set{\cT(X) = q} = \E\left[\prod_{x \in \sX}\indicator{Nq(x) =\sum_{i=1}^N\indicator{X_i = x}}\right].
}
We can write the above probability as an expectation of an integral
\EQ{
P\set{\cT(X) = q} = \E\left[\int\prod_{x \in \sX}\frac{d\lambda(x)}{2\pi}\exp\left(i\lambda(x)\left(Nq(x) - \sum_{i=1}^N\indicator{X_i = x}\right)\right)\right].
}
From the independence and identical distribution of $N$-length sequence $X$ and exchanging expectation and integral for the bounded integrand by bounded convergence theorem, we can write 
\EQ{
P\set{\cT(X) = q} =\int\prod_{x \in \sX}\frac{d\lambda(x)}{2\pi}\exp\left(i\lambda(x)Nq(x)\right) \left(\E\left[\exp\left(-i\sum_{x \in \sX}\lambda(x) \indicator{X_i = x}\right)\right]\right)^N.
}
Since the distribution of $X_i$ is $p(x)$, we have $\E\left[\exp\left(-i\sum_{x \in \sX}\lambda(x) \indicator{X_i = x}\right)\right] = \sum_{x \in \sX}p(x)e^{-i\lambda(x)}$.  
Therefore, we obtain $P\set{\cT(X) = q} =\int\prod_{x \in \sX}\frac{d\lambda(x)}{2\pi}e^{NS(\lambda)}$, 
where the exponent is called \textbf{action}
\EQ{
S(\lambda) = i \sum_{x \in \sX}\lambda(x)q(x) + \log\left(\sum_{x \in \sX}p(x)e^{-i\lambda(x)}\right).
}
For large particle limit $N \to \infty$, the probability of getting type $q$ from the sequence $X$ is dominated by the largest exponent $S(\lambda^\ast)$ where $\pd{S(\lambda)}{\lambda(x)} = 0$ for each $x \in \sX$. 
That is, 
\EQ{
iq(x) = \frac{ip(x)e^{-i\lambda^\ast(x)}}{\sum_{y \in \sX}p(y)e^{-i\lambda^\ast(y)}}
}
Let $C \triangleq \sum_{y \in \sX}p(y)e^{-i\lambda^\ast(y)}$, then we have $-i\lambda^\ast(x) = \log C + \log\frac{q(x)}{p(x)}$. 
Therefore, we obtain 
\EQ{
S(\lambda^\ast) = - \sum_xq(x)\log\frac{q(x)}{p(x)} = -D(q \Vert p). 
} 
\appendix
\section{Euler's identity}
%\section{Cauchy's integral theorem}
%Let $U$ be an open subset of the complex plane $\C$ such that $U$ contains the disk $D = \set{z \in \C: \abs{z-z_0} \le r}$. 
%Let $\gamma$ be the clockwise boundary circle of the disk $D$, then  for any holomorphic function $f: U \to \C$ and $a \in D^o$, we have
%\EQ{
%f(a) = \frac{1}{2\pi i}\oint \frac{f(z)}{z-a}dz.
%}
\begin{thm}
We can write the following equality in terms of integration with a scalar $\lambda(x)$ as 
\EQ{
\indicator{X_i = x} = \int_{0}^{2\pi}\frac{d\lambda(x)}{2\pi}\exp(i\lambda(x)(X_i- x)). 
}
Therefore, we can write the equality in terms of multi-dimensional integral in terms of vector $\lambda = (\lambda(x): x \in \sX)$ as 
\EQ{
\prod_{x \in \sX}\indicator{X_i = x} = \int\prod_{x \in \sX}\frac{d\lambda(x)}{2\pi}\exp(i\lambda(x)(X_i- x)). 
}
\end{thm}
\end{document}
