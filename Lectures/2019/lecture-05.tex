% !TEX spellcheck = en_US
% !TEX spellcheck = LaTeX
\documentclass[letterpaper,english,10pt]{article}
\input{../../header}

%opening
\title{Lecture-05: The Boltzmann Distribution}% and Thermodynamic Potentials}
%\author{Shashank Dattathri}

\begin{document}
\maketitle
\section{The Boltzmann Distribution}
The fundamental purpose of statistical physics is to understand how microscopic interactions of particles (atoms, molecules, etc.) can lead to macroscopic phenomena. 
It is unreasonable to try to calculate how each and every particle is behaving. 
Instead, we use probability and statistics to model the behaviour of a large group of particles as a whole. 

A physical system can be described probabilistically as: 
\begin{itemize}
\item A \textbf{space of configurations $\sX$}: The state/configuration of the $i^{th}$ particle is represented by the random variable $x_i \in \sX$. If there are $N$ particles, then the configuration of the system is represented by $x=\left(x_1, x_2, \dots, x_N \right)$, where each $x_i \in \sX$. The configuration space for a $N$ particle system is the product space 
$\underbrace{\sX \times \sX \times \ldots \times \sX }_{N}={\sX}^N$. 
We will limit ourselves to configuration spaces which are 
\begin{enumerate}[$(i)$]
\item finite sets, or 
\item smooth, compact, finite dimensional manifolds. 
\end{enumerate}
\item A \textbf{set of obervables} which are real-valued functions from the configuration space to $\R$. 
That is, any observable is $\cO:{\sX}^N \to \R$ such that for any configuration $x \in \sX^N$, 
we have the observable $\cO(x)$. 
%\EQ{x \mapsto \cO(x)}
A key point to note is that observables can, at least in principle, be measured through an experiment. In contrast, the configuration of a system usually cannot be measured. 
\item One special observable is the \textbf{energy function} $E(x)$. 
The form of the energy function depends on the level of interaction of the particles. 
\end{itemize}
\begin{shaded*}
\begin{exmp}[Interacting Particle System Energy]
\begin{enumerate} 
We consider three different examples of energy function for an $N$-particle system. 
\item 
An energy function of the form
%\EQ{
$E(x)=\sum_{i=1}^{N} E_i(x_i)$
%} 
is called a \textbf{non-interacting system}, as the total energy of the system depends only on the energies of the individual particles. 
\item An energy function of the form  
\EQ{
E(x)=\sum_{i=1}^{N} E_i(x_i)+ \alpha \sum_{\substack{i,j=1\\i<j}}^{N}E_{ij}(x_i,x_j)
} 
is called a \textbf{two-body interaction}, as interaction between pairs of particles is taken into account. 
\item In general, if the energy function has a term of the form 
\EQ{
\sum_{i_1,\ldots, i_k} E_{i_1,\ldots, i_k}(x_{i_1},\ldots , x_{i_k})
} 
the system has a \textbf{k-body interaction}. 
However, in real physical systems, interactions above the two or three body level are rarely considered. 
\end{enumerate}
\end{exmp}
\end{shaded*}

\begin{defn} 
Given a finite configuration space $\sX$ and energy function $E: \sX \to \R$, 
the \textbf{Boltzmann distribution} is the probability distribution for the system to be in state $x \in \sX$, 
given by
\EQ{
P\set{X = x}=\mu_{\beta}(x)=\frac{e^{-\beta E(x)}}{Z(\beta)}, 
}
where $Z(\beta)$ is the normalizing constant such that $\sum_{x \in \sX} \mu_{\beta}(x)=1$.
\end{defn}
\begin{defn}
Given a finite configuration space $\sX$ and the energy function $E : \sX \to \R$, 
the \textbf{partition function} is defined as
\EQ{
Z(\beta)=\sum_{x \in \sX} e^{-\beta E(x)}.
}
Here, the quantity $\beta$ is the inverse temperature, i.e. $\beta=1/T$. 
(In physics, $\beta$ is actually defined as 1/($k_B$T), where $k_B$ is the Boltzmann constant. Here, we simply define $k_B=1$.)
\end{defn}
The partition function is so called because it describes how particles are distributed or ``partitioned'' into different energy levels of a system.

%Note that the partition function $Z(\beta)$ normalizes the probability distribution, so that
%\EQ{\sum_{x \in \sX} \mu_{\beta}(x)=1}
\begin{shaded*}
\begin{exmp} [Independent coin tosses]
Let us consider the case of $n$ independent coin tosses with coin bias being $p$. 
We can show that for large $n$, 
the number of ways of getting $qn$ heads in $n$ coin tosses is 
\EQ{
\binom{n}{qn}p^{qn}(1-p)^{qn} \approx 2^{-n \cD(q\Vert p)} = e^{-n \cD(q \Vert p)\ln 2}.
} 
We take $n$ coin tosses as $n$ particles, and their states $x_i \in \sX = \set{H,T}$. 
Hence, the configuration space is $\sX^n = \set{H,T}^n$. 
The energy of the state $x \in \sX^n$ is  given 
\EQ{
E(x) = \frac{n}{\beta}\cD\left(\frac{1}{n}\sum_{i=1}^n\indicator{x_i = H}\Vert p\right) \ln 2.
}
%Therefore, we can obtain an ``energy function'' for the coin toss problem:
%\EQ{e^{-\beta E(q)}=2^{n \cH(q)}} 
%\EQ{E(q)=-\frac{n\cH(q) \ln 2}{\beta}}
The most probable fraction of heads is $q=p$, as we proved in class. 
We can see that the ``energy'' is minimum for this configuration. 
The extremes of $q=0$ or $q=1$, corresponding to no heads or all heads, have the maximum ``energy''.
\end{exmp}
\end{shaded*}

\begin{defn}
The \textbf{Boltzmann average} of an obervable $\cO$ is defined as the expactation value of that obervable under the Boltzmann distribution. It is denoted by $\inner{\cO}$.
\EQ{
\inner{\cO}=\sum_{x \in \sX}\cO(x) \mu_{\beta}(x)=\frac{1}{Z(\beta)}\sum_{x \in \sX}e^{-\beta E(x)} \cO(x).
}
\end{defn}
\begin{shaded*} 
\begin{exmp}[Spin-$1/2$ system]
One of the intrinsic properties of elementary particles is spin, denoted by $\sigma$. An \textbf{Ising spin} takes values in $\sigma \in \sX =\{+1, -1\}$. The energy of the particle in spin state $\sigma$ in a magnetic field B is given by $E(\sigma)=-B\sigma$. 
Since there are only two states, the partition function is 
\EQ{
Z(\beta)=\sum_{\sigma=\{+1,-1\}}e^{-\beta E(\sigma)}=e^{-\beta B}+e^{\beta B}.
} 
The probability of the particle being in spin state $\sigma$ is given by 
\EQ{
\mu_{\beta}(\sigma)=\frac{e^{-\beta E(\sigma)}}{Z(\beta)}=\frac{e^{\beta B \sigma}}{e^{-\beta B}+e^{\beta B}}.
}
The average value of the spin, called the \textbf{magnetization} is
\EQ{
\inner{\sigma}=\sum_{\sigma=\{+1,-1\}} \sigma \mu_{\beta}(\sigma)=\frac{e^{\beta B}-e^{-\beta B}}{e^{-\beta B}+e^{\beta B}}=\tanh(\beta B).
}
\begin{itemize}
\item At high temperatures ($T>>|B|$ or $\beta \to 0$), we have $e^{\beta B} \approx e^{-\beta B}$, so $\inner{\sigma} \approx 0$.  
\item At low temperatures ($\beta \to 0$), we have two cases. 

If $B>0$, then $e^{\beta B} >> e^{-\beta B}$, so $\inner{\sigma} \approx 1$. 
If $B<0$, then $e^{\beta B} << e^{-\beta B}$, so $\inner{\sigma} \approx -1$. 
That is, we have $\inner{\sigma} \approx \sign(B)$. 
\end{itemize}
\end{exmp} 
\end{shaded*}

\begin{shaded*} 
\begin{exmp}[Pott's spin]
Another spin variable is the \textbf{Potts spin} with $q$ states, 
which takes values in $\sX= [q] = \{1,2,\ldots,q\}$. 
In this case, the energy of a particle with Potts spin $\sigma$, 
when it is placed in a magnetic field of intensity $B$ pointing in direction $r$, 
is given by 
\EQ{
E(\sigma)=-B\indicator{\sigma=r}.
}
Now that we have the configuration space and the distribution function, we can calculate the relevant quantities. 
The partition function is given by 
\EQ{
Z(\beta)=e^{\beta B}+\underbrace{e^0+e^0+...+e^0}_{q-1\text{ times}}=e^{\beta B}+(q-1).
}
The Boltzmann distribution is computed to be 
\EQ{
\mu_{\beta}(\sigma)=\frac{e^{-\beta E(\sigma)}}{Z(\beta)}=\frac{e^{\beta B \mathbb{1}_{\{\sigma=r\}}}}{e^{\beta B}+(q-1)}.
}
The Boltzmann average of energy is given by 
\EQ{
\inner{ E }=-B \frac{e^{\beta B}}{e^{\beta B}+(q-1)}+(\text{all other terms are 0}) =\frac{-B e^{\beta B}}{e^{\beta B}+(q-1)}.
}
\end{exmp}
\end{shaded*}
\begin{shaded*} 
\begin{exmp}[Particles in a bottle]
Let us consider the case of a particle in a closed container (bottle), which is placed in a gravitational field. 
The space of configurations is the three-dimensional volume of the bottle, which can be represented as $\sX=\text{BOTTLE}\subset \R^3$. 

The (potential) energy of a particle in a gravitational field is $E(x)=mg h(x)=w h(x)$, where $h(x)$ corresponds to the height of the configuration $x$. 
Here, $x$ is not to be confused with the $x$-coordinate; it is simply a representation of the state of the particle. 

In this case, the partition function is difficult to calculate, because $\sX$ is an infinite set. 
However, we can still compute some related quantities in a straightforward fashion. 
Letting the partition function be $Z(\beta)$, we see that the Boltzmann density is 
\meq{3}{
&\mu_{\beta}(x)=\frac{e^{-\beta wh(x)}}{Z(\beta)}, && \mu_{\beta}(y)=\frac{e^{-\beta wh(y)}}{Z(\beta)}, &&\frac{\mu_{\beta}(x)}{\mu_{\beta}(y)}=e^{\beta w\left(h(y)-h(x) \right)}.
}
Let us fix $y=0$. 
We get 
\EQ{\frac{\mu_{\beta}(x)}{\mu_{\beta}(0)}=e^{-\beta w h(x)}}
The above equation tells us the greater the height of the particle, the smaller the probability of the particle being in that state. 
This makes physical sense, because we expect most of the particles to be at low heights due to gravity. 
Note that this result was obtained without any explicit knowledge of the partition function. 
\end{exmp}
\end{shaded*}

\section{Temperature Limits}
Let us now see that happens to the Boltzmann distribution at high and low temperatures. 
The probability of the particle being in state $x$ according to the Boltzmann distribution is 
\EQ{
\mu_{\beta}(x)=\frac{e^{-\beta E(x)}}{\sum_{x \in \sX}e^{-\beta E(x)}}.
}
At high temperatures $\beta \to 0$, and therefore we have $e^{-\beta E(x)} \to 1$ for all values of energy. 
The partition function is simply $Z(\beta)=\sum_{x \in \sX} e^{-\beta E(x)}=| \sX |$. The probability of the particle being in any state $x$ is 
\EQ{
\lim_{\beta \to 0} \mu_{\beta}(x)=\frac{1}{| \sX |}.
}
which is a uniform distribution, i.e. in the high temperature limit, we expect every state to become equally probable. 
Note that this is in accordance with the Ising spin model. 
At high temperatures, the spin of the particles become randomly orientated, so the average spin (magnetization) will be $0$. 

The case of low temperature is more interesting. 
We need the following definitions to understand the low temperature limit. 
\begin{defn}
The \textbf{ground state energy} of the system is the lowest energy attainable by it, denoted by $E_0 = \inf\{E(x): x \in \sX\}$. 
A state corresponding to the ground state energy is called a \textbf{ground state}, 
and the set of ground states is given by $\sX_0 = \set{x \in \sX: E(x) = E_0}$. 
%Mathematically, a state $x_0 \in \sX$ is a ground state if $E(x)>E(x_0)$ for all $x \in \sX$. 
\end{defn}
%Let the set of all ground states be $\sX_0$. 
In terms of the ground state $\sX_0$, we can write the Boltzmann distribution as 
\EQ{
\mu_{\beta} (x) %=\frac{e^{-\beta E(x)}}{e^{-\beta E_0} \sum_{x \in \sX} e^{-\beta \left(E(x)-E_0\right)}} \\
=\frac{e^{-\beta (E(x)-E_0)}}{|\sX_0|+\sum_{x \in \sX \setminus \sX_0} e^{-\beta (E(x)-E_0)}}
}
Note that since $E(x)>E_0$ for all $x \in \sX \setminus \sX_0$, the second term in the denominator goes to 0 as $\beta \to \infty$. The term in the numerator is 1 if $E(x)=E_0$ and it is 0 otherwise. 
Therefore, the probability distribution can be written as 
\EQ{
\mu_{\beta}(x)=\frac{1}{|\sX_0|}\indicator{x \in \sX_0}.
}
So, at low temperatures, all the particles will settle to the ground state, and all the other energy levels are left unpopulated. 
We will be primarily interested in the low temperature limit of systems, as it provides insight into the ``low energy structure'' of the system. 
At high temperatures, the states are just random, which does not give us much information.  

\section{Thermodynamic Potentials} 
In the study of the Boltzmann distribution, it is often useful to study a few functions depending on the system which summarize the system and provide useful information. 
Such functions are called \textbf{thermodynamic potentials} and are written as functions of the inverse temperature $\beta$. 
The following are the commonly used thermodynamic potentials. 
\begin{defn}
The \textbf{free energy} of a system is defined as 
\EQ{
F(\beta)\triangleq-\frac{1}{\beta} \ln Z(\beta).
}
The \textbf{free entropy} of a system is defined as 
\EQ{
\Phi(\beta)\triangleq -\beta F(\beta)=\ln Z(\beta).
}
The \textbf{internal energy} of a system is defined as 
\EQ{
U(\beta)\triangleq\frac{\partial}{\partial \beta}\left(-\Phi(\beta) \right)=\frac{\partial}{\partial \beta} \left(\beta F(\beta) \right).
}
The \textbf{cannonical entropy} of a system is defined as 
\EQ{
S(\beta)\triangleq\beta^2 \frac{\partial F(\beta)}{\partial \beta} = \beta[\frac{\partial}{\partial \beta}(\beta F(\beta)) - F(\beta)].
}
\end{defn}
Using these definitions, we derive some relations between the potentials which will be useful later. 
\begin{lem} Basic identities of thermodynamic potentials
\begin{enumerate}
\item Relation between free energy, internal energy, and canonical entropy: $F(\beta)=U(\beta)-\frac{1}{\beta}S(\beta)=-\frac{1}{\beta} \Phi(\beta)$.
\begin{proof}
From the definition, we can write 
\EQ{
U(\beta)=\frac{\partial}{\partial \beta} \left(\beta F(\beta) \right)=F(\beta)+\beta \frac{\partial F(\beta)}{\partial \beta}=F(\beta)+\frac{1}{\beta}S(\beta).
}
Rearranging, we get the required relation.
\end{proof}
\item Internal energy is the expected energy under the Boltzmann distribution. That is, $U(\beta)=\inner{ E}$.  
\begin{proof}
From the definition of free entropy, we can write 
\EQ{
U(\beta)=\frac{\partial}{\partial \beta}\left(-\Phi(\beta) \right)=\frac{\partial}{\partial \beta}(- \ln Z(\beta)) 
=-\frac{1}{Z(\beta)} \frac{\partial Z(\beta)}{\partial \beta}.
}
%Here onwards, for brevity, we simply use $\sum$ to represent $\sum_{x \in \sX}$. 
For a finite configuration space $\sX$, we can exchange the partial derivative and the summation, to write
\EQ{
U(\beta) =-\frac{1}{Z(\beta)} \frac{\partial}{\partial \beta} \sum_{x \in \sX} e^{-\beta E(x)}
=-\frac{1}{Z(\beta)} \sum_{x \in \sX} e^{-\beta E(x)} (-E(x))
=\sum_{x \in \sX} \mu_{\beta}(x) E(x)
=\inner{E}.
}
\end{proof}
\item Canonical entropy is the Shannon entropy of the Boltzmann distribution. That is, $(\beta)= {H(\mu_\beta)}{\ln 2}.$
\begin{proof}
%Using the definition of canonical entropy, we can write 
%\EQ{
%S(\beta)=\beta^2 \frac{\partial F}{\partial \beta} 
%=\beta^2 \frac{\partial }{\partial \beta} \left(-\frac{1}{\beta} \ln Z(\beta) \right)
%= \beta^2 \left( \frac{1}{\beta^2} \ln Z(\beta) - \frac{1}{\beta }\frac{1}{Z(\beta)}\frac{\partial Z(\beta)}{\partial \beta} \right). 
%}
From the result in first two parts and the definition of free entropy, we can write the canonical entropy as 
\EQ{
S(\beta) = \Phi(\beta) + \beta U(\beta) =  \ln Z(\beta) +\sum_{x \in \sX} \beta E(x) \mu_\beta (x) = \sum_{x \in \sX}\mu_\beta(x)(\beta E(x)+\ln Z(\beta)). %\hspace{10mm} \text{(this was evaluated in the last proof)} 
}
Now note that $\mu_{\beta}(x)=\frac{e^{-\beta E(x)}}{Z(\beta)}$. 
Taking natural log on both sides, we have $\ln \mu_\beta(x)= -\beta E(x)-\ln Z(\beta)$. 
Therefore, we recognize that
%Substituting for $\beta E(x)$, we have:
\EQ{
S(\beta)%&= \ln Z(\beta)+\sum \mu_{\beta} \left(-\ln \mu_\beta(x)-\ln Z(\beta) \right)  \\
=-\sum_{x \in \sX} \mu_\beta \ln \mu_\beta(x) = H(\mu_\beta)\ln 2.
}
\end{proof}
%\textbf{Thus, the canonical entropy is the Shannon entropy of the Boltzmann distribution (up to a multiplicative constant).} 
\item Second partial derivative of free entropy with respect to the inverse temperature is auto-covariance of energy under the Boltzmann distribution. 
That is, $-\frac{\partial ^2}{\partial \beta^2} \left( \beta F(\beta) \right)=\inner{ E(x)^2 }-\inner{ E(x) }^2$. 
\begin{proof} 
We can write the second partial derivative of free entropy as first partial derivative of the negative free energy, 
and hence 
\EQ{
-\frac{\partial ^2}{\partial \beta^2} \left( \beta F(\beta) \right)=-\frac{\partial}{\partial \beta} \left( U(\beta)\right) 
=-\frac{\partial}{\partial \beta} \left( \frac{\sum_{x \in \sX} e^{-\beta E(x)} E(x)} {Z(\beta)}\right)
=\sum_{x \in \sX} \mu_\beta(x) E(x)^2-\sum_{x \in \sX}\mu_\beta(x)E(x) \frac{1}{Z(\beta)}\frac{\partial Z(\beta)}{\partial \beta}.
}
We recall that $\frac{\partial \ln Z(\beta)}{\partial \beta} = \inner{E}$, to get the result.
%\EQ{
%-\frac{\partial ^2}{\partial \beta^2} \left( \beta F(\beta) \right) =\inner{E^2}-\inner{E}^2.
%}
\end{proof}
\item The free entropy function $\Phi(\beta)$ is convex in $\beta$.
\begin{proof}
Recall that the free entropy $\Phi(\beta) = -\beta F(\beta)$. 
\begin{align*}
- \frac{\partial^2}{\partial \beta^2} \Phi(\beta) & - \frac{\partial^2}{\partial \beta^2} (\beta F(\beta)) \\
	&= \langle E^2 \rangle - \langle E \rangle^2 \\
	&= Var_{\mu_\beta}(E) \geq 0
\end{align*}
and hence $\Phi$ is convex.
\end{proof}
\end{enumerate}
\end{lem}
\end{document}