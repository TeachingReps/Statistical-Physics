% !TEX spellcheck = en_US
% !TEX spellcheck = LaTeX
\documentclass[letterpaper,10pt,english]{article}
\input{../../header}
\title{Lecture-01: Random Variables and Entropy}
\author{}

\begin{document}
\maketitle

\section{Random Variables} 

Our main focus will be on the behavior of large sets of discrete random variables. 
%From a mathematical perspective this class deals primarily with probability and, in particular, discrete random variables. 
\begin{defn} 
A \textbf{discrete random variable}, $X$, is defined by following information: 
(i) $\sX$ : the finite set of values that it may take, 
(ii) $p_X : \sX \to [0, 1]$: the probability it takes each value $x \in X$ . 
Of course, the probability distribution $p_X$ must satisfy the normalization condition $\sum_{x \in X}p_X (x) = 1$. 
If there is no ambiguity, we may use $p(x)$ to denote $p_X (x)$. 
\end{defn}
\begin{shaded*}\begin{exmp}
Let the random variable $X$ denote the sum of two fair 6-sided dice. 
Then, $\sX = \set{2, 3, \dots, 12}$ and 
\EQ{
p_X (x) = \frac{6 - \abs{7 - x}} {36}.
}
\end{exmp}\end{shaded*}
\begin{defn}
An \textbf{event} $A \subseteq \sX$ is a subset of values. 
The probability of an event is denoted 
\EQ{
\P(X \in A)=\P(A) = \sum_{x \in A}p_X(x) = \sum_{x \in A}\P(X =x).
}
Also, an event is sometimes defined in words, $A = ``X \text{ is even}''$.
\end{defn}
\begin{shaded*}\begin{exmp} 
If X is the sum of two fair 6-sided dice and $A = ``X \text{ is even}''$. Then, 
\EQ{
\P(X \text{ is even}) = \P(A) = \sum_{x \in A}p_X (x) = \frac{1 + 3 + 5 + 5 + 3 + 1}{36} = \frac{1}{2}.
}
\end{exmp}\end{shaded*}
\begin{defn} 
For a discrete random variable, the expected value (or average) of $f : \sX \to \R$ is denoted
\EQ{
\E f = \E{f(X)} = \sum_{x \in \sX}p_X(x)f(x).
}
Mathematically, $\E{}$ can be seen as a linear operator from the space of real functions on $\sX$ to the set of real numbers. 
Thus,
\EQ{
\E{af(X) + bg(X)} = a\E{f(X)} + b\E{g(X)}.
}
\end{defn}

\begin{shaded*}\begin{exmp}
If $X$ is the sum of two fair 6-sided dice and $f(x) = (x - 7)^2$, 
then 
\EQ{
\E{(X-7)^2} =\sum_{x \in A}p_X(x)(x-7)^2 = \frac{2(1\cdot 5^2 +2\cdot 4^2 +3\cdot 3^2 +4\cdot 2^2 +5\cdot 1^2)}{36} = \frac{105}{18}.
}
Since the mean is $\E{X} = 7$, this actually equals the variance of $X$.
\end{exmp}\end{shaded*}
\begin{defn} 
A continuous random variable, $X$, taking values on the set $\sX = \R^d$ or in some smooth finite-dimensional manifold is defined by its cumulative distribution function $\P(X \le x)$, where $X \le x$ is used to denote
$X_i \le x_i$ for $i = 1, \dots, d$. For such a r.v., the probability measure with respect to the
infinitesimal element $dx$ is denoted by $dp_X (x)$. 
For a measurable event $\sA \subseteq X$ , this gives 
\EQ{
\P(X \in \sA) = \int_{\sA}dp_X(x) = \int\indicator{x \in \sA}dp_X(x),
}
where the indicator function $\indicator{s}$ is $1$ if the logical statement $s$ is true and $0$ otherwise. 
If $p_X$ admits a density, with respect to Lebesgue measure, then it will be denoted by $p_X(x)$. 
In this case, we can write
\EQ{
\P(X \in \sA) = \int_{\sA}p_X(x)dx = \int\indicator{x \in \sA}p_X(x)dx.
}
\end{defn}
\begin{shaded*}\begin{exmp}
If $X$ is a continuous random variable defined, for $a, b \in \R$ with $a < b$, by
\EQ{
\P(X \le x) = 
\begin{cases}
0 &  x < a\\
\frac{x-a}{b-a} & a \le x \le b\\
1 & x > b,
\end{cases}
}
then it is uniform on $[a, b]$ and its density is given by $p_X (x) = \frac{1}{b-a}\indicator{x \in [a,b]}$.
\end{exmp}\end{shaded*}

\begin{defn} 
The expected value and variance of a function $f : \R^d \to \R$ of a continuous
random variable $X \in \R^d$ are given by
\eq{
&\E f = \E{f(X)} = \int f(x)dp_X(x),\\
&\Var{f} =\Var{f(X)}=\E{(f(X)- \E{f(X)})^2}=\E{f(X)^2} -\E{f(X)}^2.
}
\end{defn}
\begin{shaded*}\begin{exmp} 
If $X$ is a continuous random variable that is uniform on $[a, b]$, 
then its mean and variance are given by
\eq{
&\E{X} = \int_a^b \frac{x}{b-a}dx = \frac{b^2-a^2}{2(b-a)} = \frac{b+a}{2},\\
&\Var{X} =  \int_a^b \frac{x^2}{b-a}dx -\left(\frac{b+a}{2}\right)^2= \frac{b^3-a^3}{3(b-a)} -\left(\frac{b+a}{2}\right)^2= \frac{(b-a)^2}{12}.
}
\end{exmp}\end{shaded*}
     
\section{Entropy}
In statistical mechanics, the entropy is proportional to the logarithm of the number of resolvable microstates associated with a macrostate. 
In classical mechanics, this quantity contains an arbitrary additive constant associated with the size of a microstate that is considered resolvable. 
In quantum mechanics, there is a natural limit to resolvability and this constant is related to the Planck constant. 
For random variables, Shannon chose the following definition which is similar in spirit. 
\begin{defn} 
The \textbf{entropy} (in bits) of a discrete random variable $X$ with probability distribution $p(x)$ is denoted
\EQ{
H(X) \triangleq -\sum_{x \in \sX}p(x)\log_2p(x)=\E{\frac{1}{\log_2p(X)}},
}
where $0 \log_2 0 = 0$ by continuity. 
The notation $H (p)$ is used to denote $H (X )$ when $X \sim p(x)$. 
When there is no ambiguity, $H$ will be used instead of $H (X)$. 
The unit of entropy is determined by the base of the logarithm with base-2 resulting in ``bits'' and the natural log (i.e., base-e) resulting in ``nats''. 
\end{defn}
\begin{rem}
Roughly speaking, the entropy $H (X)$ measures the uncertainty in the random variable $X$.
\end{rem}
\begin{shaded*}\begin{exmp} 
If $X$ is uniform, then $p(x) = \frac{1}{\abs{\sX}}$ and 
\EQ{
H (X) = \E{\log_2 \frac{1}{\abs{\sX}}} = \log_2 \abs{\sX}.
}
Choosing $\abs{\sX} = 2$, we see that a uniform random bit has exactly $\log_2 2 = 1$ bit of entropy.
\end{exmp}\end{shaded*}
\begin{shaded*}\begin{exmp}
Let $X$ be a binary r.v. defined by $p(0) = 1-q$ and $p(1) = q$. 
In this case, we have
\EQ{
H(X)= \cH(q) = q\log_2 \frac{1}{q}+(1-q)\log_2\frac{1}{1-q}, 
}
where $\cH(q)$ is called the binary entropy function. 
This function is concave and symmetric about $q = \frac{1}{2}$. 
It also satisfies $\cH(0) = \cH(1) = 0$ and $\cH(1/2) = 1$.
\end{exmp}\end{shaded*}
\begin{shaded*}\begin{exmp}
The number of length-$n$ binary sequences with exactly $qn$ ones is given by $\binom{n}{qn}$. 
Using Stirling's formula, $n! = \sqrt{2\pi n}(\frac{n}{e})^n(1 + O(\frac{1}{n})$, we see that 
\eq{
\binom{n}{qn} &= \frac{n!}{(n-qn)!(qn)!}\\
&= \frac{\sqrt{2\pi n}(\frac{n}{e})^n(1+O(\frac{1}{n})}{\sqrt{2\pi n(1-q)}(\frac{n(1-q)}{e})^n(1+O(\frac{1}{n(1-q)})\sqrt{2\pi qn}(\frac{qn}{e})^n(1+O(\frac{1}{qn})}\\
&= \frac{1}{\sqrt{2\pi n q(1-q)}}2^{n\cH(q)}\left(1 + O\left(\frac{1}{nq(1-q)}\right)\right).
}
\end{exmp}\end{shaded*}
\begin{rem} 
This shows that the binary entropy determines the exponential growth rate of the number of binary sequences with a fixed fraction of ones. 
In fact, this is a fundamental property of the entropy. 
More generally, we will see that the entropy $H (X)$ is the exponential growth rate of the number of length-$n$ sequences (i.e., there are roughly $2^{nH(X)}$ such sequences) where the fraction of $x's$ converges to $np(x)$. 
This also implies that $nH (X)$ is essentially equal to the minimum number of binary digits required to index all length-$n$ sequences of this type. 
\end{rem}
\begin{lem} 
Basic properties of entropy:
\begin{enumerate}
\item (non-negativity) $H (X) \ge 0$ with equality iff $X$ is constant. 
\begin{proof} 
%For each $x \in \sX$, we have $\log_2\frac{1}{p(x)} \ge 0$ and, 
If $X$ is not constant, there is an $x_0 \in \sX$ with $p(x_0) \in (0,1)$. 
Thus, 
\EQ{
H (X) \ge p(x_0) \log_2(1/p(x_0)) \ge 0.
}
\end{proof}
\item (decomposition rule) For any partition $A = (A_1, A_2, \dots, A_m)$ of $\sX$, we have 
\EQ{
H(p) = H(p_A) + \sum_{i=1}^m p(A_i)H(p_i), 
}
where we define $p_A(i) = p(A_i) = \sum_{x \in A_i} p(x)$ for $i \in [m]$ and $p_i(x) = \frac{p(x)}{p(A_i)}$ for $x \in A_i$.
\begin{proof}
Observe that
\EQ{
H(X) = \sum_{i=1}^m\sum_{x \in A_i}p(x)\log_2\frac{1}{p(x)} = H(p_A) + \sum_{i=1}^mp(A_i)\sum_{x \in A_i}\frac{p(x)}{p(A_i)}\log_2\frac{p(A_i)}{p(x)} 
}
\end{proof}
\end{enumerate}
\end{lem}
\begin{shaded*}\begin{exmp} 
Compute the entropy of the distribution $p(x) = \begin{bmatrix}0.125  & 0.375 & 0.25 &0.25\end{bmatrix}$. Using decomposition with $A_1 = \set{1, 2}$ and $A_2 = \set{3, 4}$, we get 
\EQ{
H (p) = 1 + 0.5\cH(1/4) + 0.5 \approx 1.9056.
}
\end{exmp}\end{shaded*}
\end{document}
