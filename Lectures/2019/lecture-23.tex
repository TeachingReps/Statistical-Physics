% !TEX spellcheck = en_US
% !TEX spellcheck = LaTeX
\documentclass[letterpaper,english,10pt]{article}
\usepackage[utf8]{inputenc}

\title{lecture 23}

\input{../../header}
%\include{header}
\begin{document}
\maketitle

\section{Derivation Of Boltzmann Distribution}

The probability that a system is found in a particular state is given by the celebrated Boltzmann distribution.This section gives a formal derivation to this definition.\\\\
Let $p_i$ be the probability that a particle or a subsystem is in state i. This will imply that the enrtopy of the system is,
\EQ{
H(p)=- \sum_i p_i \log p_i
}
Now for this system there are two constraints.The first one is the universal constraint which is basic probability constraint given by,
\EQ{
 \sum_i p_i = 1 
}
The second constraint is the average energy constraint where the average energy of the particles is a constant.Thus the second constraint translate formally to,
\EQ{
 \sum_i p_i E_i= E
}
where $E_i$ is the energy of the $i^{th}$ subsystem and E is the average energy.\\\\
According to the \textbf{principle of maximum entropy} , if nothing is known about
distribution except that it belongs to a certain class (usually defined in terms of specified
properties or measures), then the distribution with the largest entropy should be chosen as
the least-informative default. The motivation is two fold
\begin{enumerate}
    \item Maximizing entropy minimizes the amount of prior information built into the distribution.
    \item Many physical systems tend to move towards maximal entropy configurations over time.
\end{enumerate}
Now in-order to derive the probability distribution of the system we maximize the entropy for
the reasons mentioned above given the two constraints.Maximizing $H(p)$ is same as minimizing
$F(p)=-H(p)$. So the minimizing problem can be formulated as:
\EQ{
\min_{pi}F(p)\quad  
\text{subject to:}\quad 1.\sum_i p_i = 1 \quad  2.\sum_i p_i E_i= E
}
For solving the above problem we use the method of Legrange multipliers.Taking $\alpha$ and
$\beta$ as the two constant multipliers we write:
\EQ{
F(p) + \alpha (\sum_i p_i - 1) + \beta(\sum_i p_i E_i - E) = F'(p)
}
Taking derivative of the above function with respect to $p_i$ (PS: minimizing is done with
respect to $p_i$), we get,
\EQ{
\frac{\partial{F'(p)}}{\partial {p_i}} = \log{p_i} + 1 + \alpha + \beta E_i = 0
}
\EQ{
p_i = e^{-(1+\alpha)}e^{-\beta E_i}.
}
Taking $Z = e^{(1+\alpha)}$ we get,
\EQ{
p_i = \frac{1}{Z}e^{-\beta E_i}
}
For completing the derivation of the distribution we are required to find the value of $\beta$ as well as $Z$.By applying the first constraint, $\sum_i p_i = 1$ we get, 
\EQ{
Z(\beta) = \sum_i{e^{-\beta E_i}}
}
This $Z(\beta)$ is known as the \textbf{partition function}.Now the only remaining task is to evaluate the constant $\beta$.For this we take a look at the \textbf{definition of temperature} in thermodynamics. In thermodynamics temperature is defined as the \textbf{rate of change of energy with unit change in entropy}. 
\EQ{
T = \frac{\partial E}{\partial H}
}
For finding temperature we need to evaluate E and H. For this we make use of the second constraint $\sum_i p_i E_i= E$. From this we get ,
\EQ{
\sum_i \frac{1}{Z}e^{-\beta E_i} E_i = E
}
Using the observation, 
\EQ{
\frac{\partial Z}{\partial \beta} = - \sum_i e^{-\beta E_i} E_i
}
we get,
\EQ{
E = - \sum_i \frac{1}{Z}e^{-\beta E_i} E_i =- \frac{\partial \log{Z(\beta}}{\partial \beta}
}
Now, entropy H has to be evaluated for the derived $p_i$. 
\EQ{
H(p)= - \sum_i p_i \log p_i = \sum_i \frac{1}{Z}e^{-\beta E_i} [\beta E_i + \log Z]
}
On expanding we get,
 \EQ{
H(p) = \beta E + \log{Z} 
 }
For finding $\beta$ we find temperature$ T = \frac{\partial E}{\partial H} $
\EQ{
\partial H = \beta \partial E + E \partial \beta + \frac{\partial \log{Z(\beta)}}{\partial \beta} \partial \beta
}
Therefore temperature,
\EQ{
T = \frac{\partial E}{\partial H} = \frac{1}{\beta}
}
Thus we have evaluated the value of $\beta$. Hence the result, 

\EQ{
p_i = \frac{1}{Z}e^{-\beta E_i} ,\quad \text{where} \quad Z(\beta) = \sum_i{e^{-\beta E_i}} ,\quad  \beta = \frac{1}{T}
}

\section{CDMA in a nutshell}

CDMA stands for Code Division Multiple Access.As the name suggests it is a channel access method which makes use of "codes". Multiple users occupy the same band by having different codes. The information sequence generated by a user is modulated with a spreading code before sending it to the channel.The spreading code design is one of the major tasks in CDMA system design.The ideal characteristics of the spreading codes are as follows:
\begin{enumerate}
    \item Spreading code of each user should be orthogonal to other users' spreading code.
    \item The auto-correlation of the spreading code (with respect to its shifted versions) should be 0.
\end{enumerate}
There are two types of CDMA techniques namely Frequency-hopping spread spectrum and Direct sequence spread spectrum. The idea behind frequency hopping is to transmit data across a broad spectrum where the frequency can be rapidly switched from one to another.For rapid switching the system makes use of pseudo random sequence generators. The former technique is the one described in the beginning of this section.\\
The main application of CDMA is in the GPS systems.It is also employed in 3G,4G systems.

\section{System Model}
Consider a fully synchronous K-user CDMA system. The received signal corresponding to a spreading sequence is given by,
\EQ{
r^\mu = \frac{1}{\sqrt{N}}\sum_{k=1}^{K}s_k^\mu x_k + \sigma_0n^\mu 
}
where, $x_k \in \{-1,1\}$ is the information bit of $k^{th}$ user, $\{s_k^\mu$ ; $\mu = 1,2,...,N\}$ is the spreading sequence of $k^{th}$ user and $N$ is the number of spreading sequences.\\\\
Assume AWGN channel : $n^\mu \sim \mathcal{N}(0,1)$. $\sigma_0$ is the amplitude of the noise and $\frac{1}{\sqrt{N}}$ is the normalization factor.\\\\
Assume a random spreading model where $\{s_k^\mu$ ; $\mu = 1,...,N$ ; $k=1,...,K\}$ are iid random variables. For representing the communication model, consider the following definitions.

\begin{align*}
\mathbf{r} \:\equiv& \;[\;r^1,\;.\;.\;.\;,\;r^N\;]^T \\  \mathbf{s_k}\; \equiv&\; [\;s_k^1,\;.\;.\;.\;,\;s_k^N\;]^T,\qquad k=1,\;.\;.\;.\;,\;K\\ \mathbf{n}\; \equiv&\; [\;n^1,\;.\;.\;.\;,\;n^N\;]^T\\\mathbf{x} \;\equiv&\; [\;x_1,\;.\;.\;.\;,\;x_K\;]^T
\end{align*}   
The communication model, hence can be represented as,
\EQ{
r = \frac{1}{\sqrt{N}}Sx +\sigma _0n
}
where $S \equiv [s_1,...,s_K]$\\\\
The conditional distribution of the received data $r$, conditioned on the information bits $x$ for given spreading sequence $S$, is given by,
\EQ{
    p_0(r|x,\:S) = p_{\sigma _0n}(r-\frac{1}{\sqrt{N}}Sx)
}
Since $\sigma _0n \sim \mathcal{N}(0,\sigma_0^2)$,
\EQ{
    p_0(r|x,\:S) = (2\pi \sigma_0^2)^{-N/2}exp\big{(}-\frac{1}{2\sigma_0^2}\|r-N^{-1/2}S_x\|^2\big{)}
}
For large system limit, consider infinite number of users and spreading codes, ie. $K\rightarrow \infty$, $N\rightarrow \infty$, while
\EQ{
\frac{K}{N} = \beta \quad \text{,is kept constant}
}
Posterior distribution $(p(x|r,\:S))$ is the main quantity used for analyzing the multiuser detection problem.\\\\
Assume that the true amplitude $\sigma_0$ remains unknown, $\sigma$ is used as the control parameter instead of $\sigma_0$. Therefore the posterior distribution is,
\EQ
{
\begin{split}
p(x|r,\:S) &= \frac{p(r|x,\:S).p(x)}{\sum_{x} p(r|x,\:S).p(x)} \qquad \text{(By Bayes theorem
)}\\ &= \frac{p(x).exp\big{(}-\frac{1}{2\sigma^2}\|r-N^{-1/2}Sx\|^2\big{)}}{\sum_{x} p(x
).exp\big(-\frac{1}{2\sigma^2}\|r-N^{-1/2}Sx\|^2\big)}
\end{split}
}
Denote $Z(r,s) = \sum_{x}p(x).exp\big(-\frac{1}{2\sigma^2}\|r-N^{-1/2}Sx\|^2\big)$.\\\\
$Z(r,s)$ is the normalizing coefficient and is called the partition function.
\section{Multiuser Detectors}
\subsection{Marginal Posterior Mode (MPM) detector}
Here the estimate of the information bit for the $k^{th}$ user is given by,
\EQ{
\hat{x_k}^{(MPM)} = arg \max_{x_k} \sum_{x|x_k}p(x|r,S) \quad k=[K]
}
Here the summation is over all $x_j, j \neq k$ (marginalizing the posterior distribution over all $x_j, j \neq k$).The MPM detector is used when the control parameter $\sigma$ is unknown. 

\subsection{Jointly Optimal (JO) multiuser detector}

The estimate of the received \textbf{vector} is given by,
\EQ{
\hat{x}^{(JO)} = arg \max_{x} p(x|r,S) 
}
Here the maximal value of posterior probability is found for the vector as a whole.This kind of jointly optimal decoding is performed when the value of the control parameter $\sigma$ tends to 0.($\sigma \to 0$)

\subsection{Individually Optimal (IO) multiuser detector}
This detector is very similar to the MPM detector, but the difference here is that the control parameter $\sigma$ is known. Taking $\sigma$ as $\sigma_0$ we get the estimate of the data bit for the $k^{th}$ user as 
\EQ{
\hat{x_k}^{(MPM)} = arg \max_{x_k} \sum_{x|x_k}p(x|r,S)|_{\sigma=\sigma_0} \quad k=[K]
}

As a general observation we can see that the MPM detector is jsut a single parameter extension of JO and IO detectors. 
\subsection{Performance Measure}
The performance measure for multiuser detectors is done by calculating the bit error rate $P_b$.Let $x_0$ represent the true information-bit vector and $\hat{x}$ be the estimate output by the detector.Define:
\EQ{
d = \frac{1}{K}\sum_{k=1}^{K}x_{0k}\hat{x_k}=\frac{1}{K}x_0 \hat{x}
}
The quantity d is nothing but the \textit{correlation} of the detector output.\\\\
\textbf{Claim: } The bit error rate $P_b$ = $\frac{1-d}{2}$
 \begin{proof}
\EQ{
\begin{split}
d &= \frac{1}{K}\sum_{k=1}^{K}x_{0k}\hat{x_k} \\&= \frac{1}{K}(\sum_k{\indicator{x_{0k}
=\hat{x_k}}}-\sum_k{\indicator{x_{0k}\neq \hat{x_k}}})
\end{split}
}
\EQ{
\frac{1-d}{2} = \frac{1}{2}\Big[1-\frac{1}{K}(\sum_k{\indicator{x_{0k}=\hat{x_k}}}-\sum_k{\indicator{x_{0k}\neq \hat{x_k}}})\Big] = \frac{1}{2} \Big[\frac{K-\sum_k{\indicator{x_{0k}=\hat{x_k}}}+\sum_k{\indicator{x_{0k}\neq \hat{x_k}}}}{K}\Big]
= \frac{1}{2}\Big[\frac{2\sum_k{\indicator{x_{0k}\neq \hat{x_k}}}}{K} \Big]
}
Therefore,
\EQ{
\frac{1-d}{2} =\Big[\frac{\sum_k{\indicator{x_{0k}\neq \hat{x_k}}}}{K} \Big]
}
The above equation is nothing but the fraction of bits that are wrongly decoded. Thus the claim,
\EQ{
P_b = \frac{1-d}{2}
}
\end{proof}

\section{Large system CDMA}
As mentioned before the large system analysis is done for very high number of users as well well large number of spreading codes i.e. the limit when K,N $\rightarrow$ $\infty$. The free energy per user is defined as:
\EQ{
F_K(r,S) = K^{-1} \log{Z(r,S)}
}
One basic assumption is that the above mentioned free energy per user is self averaging in the limit K tending to infinity (with respect to the randomness of the spreading code).The formal statement follows.\\
 
\textbf{Assumption: } \textit{The limit $F \triangleq \lim_{K \to \infty} F_K(r,S)$ exists and it is equal to its average for almost all realizations of the spreading sequences and the noise.}\\
We have,\\
\EQ{
F = \lim_{K \to \infty} K^{-1} \int_{R^{N}}\overline{p_0(r/S)\log{Z(r,S)}}dr \quad (dr = \prod_{\mu=1}^{N}dr^{\mu})
}
 
The over-bar represents the averaging with respect to the randomness of the spreading codes and $p_0(r/S)$ is the probability of observing r as the output of the true channel, for the given spreading sequences S. The true channel corresponds to the one where the value of the control parameter ($\sigma_0$) is known.Therefore we have,
\EQ{
p_0(r/S)=\sum_{x_0}p(x_0)p_0(r/x_0,S) \triangleq Z_0(r,S)/C 
}
where,
\EQ{
Z_0(r,S) = \sum_{x_0} p(x_0).exp\big{(}-\frac{1}{2{\sigma_0}^2}\|r-N^{-1/2}Sx_0\|^2\big{)}
}
and the normalization constant C is :
\EQ{
C = \int_{R^{N}} Z_0(r,S) dr = {(2\pi \sigma_0)}^{N/2}
} 
This constant C is independent of the spreading sequences.\\\\

The free energy contains all the information about the statistics of the system. Therefor its necessary to evaluate the free enrgy.The free energy evaluation invloves highly complicated calculations such as finding the expectation of logarithm of functions. This would require the need of a very important analysis in statistical mechanics, namely \textit{replica analysis}  



\section{Replica Method}

Here, there is a problem for calculating the posterior average;
\EQ{
 a(\textbf{y,s}) \equiv  = \sum_{\textbf{x}}a(\textbf{x,y,s})p(x \mid \textbf{y,s}),
}

where $\textbf{x}$ is a random vector representing configurations of the underlying probability model, $\textbf{y}$ is another random vector representing observables, $\textbf{s}$ is a set of parameters, $a(\textbf{x,y,s})$ is a function of $\textbf{x, y}$ and $\textbf{s}$, and $p(\textbf{x}\mid\textbf{y,s})$ is an assumed posterior distribution of $\textbf{x}$ conditioned on the values of $\textbf{y}$ the observables and the parameters $\textbf{s}$. The posterior distribution is to be derived of from Bayes theorem, with an assumed prior distribution $p(\textbf{x})$ of $\textbf{x}$, and an assumed conditional distribution $p(\textbf{y} \mid \textbf{x,s})$ of $\textbf{y}$ on $\textbf{x}$ , parametrized by $\textbf{s}$, as

\EQ{
    p(\textbf{x} \mid \textbf{y,s}) = \frac{ p(\textbf{y} \mid \textbf{x,s})}{\sum_{x} p(\textbf{y} \mid \textbf{x,s})}
}
The posterior average $a(\textbf{y,s})$ depends on the observables $\textbf{y}$ and, quite often, they are also random variables. In such cases, over all possible the quantity of interest is the average of $a(\textbf{y,s})$ observed values, that is,
\EQ{
    E_{\textbf{y}}[a(\textbf{y,s})]
}

where $E_{\textbf{y}}[.]$ denotes expectation with respect to the true distribution of $\textbf{y}$, which may or may not be equal to the assumed one.

We now consider the case in which the parameters $\textbf{s}$ are also random, and are assumed to be independent of $\textbf{x}$. This is actually the case for the CDMA multiuser detection problem under the
random spreading assumption. Let  $\textbf{x,y}$, and $\textbf{s}$ be the information bit vector, the received signals, and the spreading sequences of all users, respectively. For every $\textbf{s}$, one can in principle calculate the average bit-error rate. One still has to average the result over the randomness of the spreading sequences to obtain the desired result. In general, we want to evaluate quantities like
\EQ{
    E_{\textbf{y,s}}[a(\textbf{y,s})]
}

In statistical-mechanics literature, the average over $\textbf{y}$, and $\textbf{s}$ is called the \textit{quenched average}. When one first takes the average with respect to the posterior distribution, one has to quench the values of $\textbf{y}$, and $\textbf{s}$ . The result is then averaged over $\textbf{y}$, and $\textbf{s}$ . Very often, one encounters difficulties in the evaluation of the quenched average. This becomes evident by rewriting the quenched average as

\EQ{
\begin{split}
      E_{\textbf{y,s}}[a(\textbf{y,s})] &=   E_{\textbf{y,s}}[\sum_{\textbf{x}}a(\textbf{x,y,s})p(\textbf{x} \mid \textbf{y,s})] \\ &= E_{\textbf{y,s}}[\frac{\sum_{\textbf{x}}a(\textbf{x,y,s})p(\textbf{y} \mid \textbf{x,s})p(\textbf{x})}{\sum_{x} p(\textbf{y} \mid \textbf{x,s})p(\textbf{x})}]
\end{split}
}

One now has to take the expectation of a ratio of random variables $\sum_{\textbf{x}}a(\textbf{x,y,s})p(\textbf{y} \mid \textbf{x,s})$ and $\sum_{x} p(\textbf{y} \mid \textbf{x,s})$ is in general difficult. A related quantity is the \textit{annealed average}, or the annealed approximation to the quenched average, which is given by taking the expectation for the denominator and the
numerator separately, that is,

\EQ{
 \frac{E_{\textbf{y,s}}[\sum_{\textbf{x}}a(\textbf{x,y,s})p(\textbf{y} \mid \textbf{x,s})p(\textbf{x})]}{E_{\textbf{y,s}}[\sum_{x} p(\textbf{y} \mid \textbf{x,s})p(\textbf{x})]}
}

The calculation of the annealed average is usually much simpler than that of the quenched average, so that the annealed average is often used as an approximation to the quenched average, but the final results are in general different from each other.

Let us define the moment generating function of $a(\textbf{x,y,s})$, or the partition function, of the (unnormalized) measure $p(\textbf{y} \mid \textbf{x,s})$ of $\textbf{x}$, given $\textbf{y}$ and $\textbf{s}$, as

\EQ{
Z(h;\textbf{y,s}) = \sum_{x} e^{h.a(\textbf{x,y,s})}p(\textbf{y} \mid \textbf{x,s})p(\textbf{x})
}

If we could evaluate the average

\EQ{
E_{\textbf{y,s}}[\log{Z(h;\textbf{y,s})}]
}

we would obtain the desired quenched average by

\EQ{
E_{\textbf{y,s}}[a(\textbf{y,s})] = \frac{\partial}{\partial h}E_{\textbf{y,s}}[\log{Z(h;\textbf{y,s})}] \mid _{h \to 0}
}

by exchanging the order of the expectation and the differentiation by \textit{h}. Of course, this simple manipulation does not help resolve the difficulty we are faced with. The difficulty we encounter here is that the evaluation of the average $E_{\textbf{y,s}}[\log{Z(h;\textbf{y,s})}]$ is often hard. Now we have to average the \textit{logarithm} of a sum of random variables, which prevents us in most cases from evaluating it analytically. Also, it is usually computationally hard. The summation may be over a very large number of terms in large-sized problems. Again, it is generally far easier to evaluate the average instead

\EQ{
E_{\textbf{y,s}}[{Z(h;\textbf{y,s})}]
}

but the calculation
\EQ{
\frac{\partial}{\partial h}\log{E_{\textbf{y,s}}[{Z(h;\textbf{y,s})}]}_{h \to 0}
}

only yields the annealed average, which is, in general, the wrong answer.
The replica method is a way to evaluate $E_{\textbf{y,s}}[\log{Z(h;\textbf{y,s})}]$. It makes use of the identity

\EQ{
E_{\textbf{y,s}}[\log{Z(h;\textbf{y,s})}] = \lim_{n \to 0} \frac{\partial}{\partial n}\log{E_{\textbf{y,s}}[\{Z(h;\textbf{y,s})\}^n]}
}

and replaces the evaluation of the average $E_{\textbf{y,s}}[\log{Z(h;\textbf{y,s})}]$ with that of

\EQ{
E_{\textbf{y,s}}[\{Z(h;\textbf{y,s})\}^n]
}

It should be noted that the quantity $[Z(h;\textbf{y,s})]^n$ is well-defined for real \textit{n}. In applying the replica method, however, one
usually evaluates $E_{\textbf{y,s}}[\{Z(h;\textbf{y,s})\}^n]$ for positive integer \textit{n} only, by regarding $[Z(h;\textbf{y,s})]^n$ as the partition function of the probability model consisting of \textit{n} independent and identical replicas of the original probability model for a given $\textbf{y}$ and $\textbf{s}$, that is,

\EQ{
[Z(h;\textbf{y,s})]^n = \sum_{x_1, ...... x_n} {e^{h.a(\textbf{x,y,s})}}\prod_{i=1}^{n}[p(\textbf{y} \mid \textbf{x}_{i},\textbf{s})p(\textbf{x}_{i})]
}

Since the replica method has been developed in statistical mechanics, it has been extensively applied to the analysis of models in the large-system (thermodynamic) limit. In such cases, the replica method is used in conjunction with the saddle-point method, or Varadhan’s theorem for asymptotics of integrals. Let \textit{K} be a parameter representing the size of a problem , and let us further assume that $a(\textbf{x,y,s})$
scales with \textit{K} so that the actual quantity of interest is the asymptotic value of the quenched average of $K^{-1}a(\textbf{x,y,s})$ as $K \to \infty$. In this case, we have to evaluate the free energy defined as
\EQ{
\mathscr{F}\equiv \lim_{K \to \infty}{K^{-1} E_{\textbf{y,s}}[\log{Z(h;\textbf{y,s})}]}
}
    The replica method replaces the right-hand side of above equation as
\EQ{
  \mathscr{F} = \lim_{K \to \infty}{K^{-1} \lim_{n \to 0} \frac{\partial}{\partial n}\log{E_{\textbf{y,s}}[\{Z(h;\textbf{y,s})\}^n]}}
}

The approach of combining the replica method with the saddle point method is efficient if the following conditions are met.

The two limits $\lim_{K \to \infty}$ and $\lim_{n \to 0}$ can be interchanged, so that the equality
\EQ{
  \mathscr{F} = \lim_{n \to 0} \frac{\partial}{\partial n}\lim_{K \to \infty}{K^{-1}\log{E_{\textbf{y,s}}[\{Z(h;\textbf{y,s})\}^n]}}
}
holds.

Dependence on $\textbf{x}$ of the function
\EQ{
e^{h.a(\textbf{x,y,s})}\prod_{i=1}^{n}[p(\textbf{y} \mid \textbf{x}_{i},\textbf{s})]
}

is expressed in terms of a set of parameters $q = q\{\textbf{x}_{i}\}$ (which may be functionals) and the average of the function over $\textbf{y}$ and $\textbf{s}$ can be performed analytically \textit{for fixed} \{$\textbf{x}_{1}, ...., \textbf{x}_{n}$\}, yielding a result of the form $\exp[{Kg(q)}]$ where $g(q)$ is a continuous function of $\textit{q}$ whose supremum is finite.

The probability measure $\mu_{K}(q)$ of the parameters $q = q\{\textbf{x}_{i}\}$, induced by the prior
$\prod_{i =1}^{n} p({\textbf{x}_{i}})$, has a large deviation property with a rate function $\mathscr{I}(q) $ as $K \to \infty$. This enables us to evaluate ${K^{-1}\log{E_{\textbf{y,s}}[\{Z(h;\textbf{y,s})\}^n]}}$ asymptotically using Varadhan’s theorem, as

\EQ{
\begin{split}
    K^{-1}\log{E_{\textbf{y,s}}[\{Z(h;\textbf{y,s})\}^n]} &= K^{-1}\log{E_{\textbf{y,s}}[\sum_{x_1, ...... x_n} {e^{h.a(\textbf{x,y,s})}}\prod_{i=1}^{n}\{p(\textbf{y} \mid \textbf{x}_{i},\textbf{s})p(\textbf{x}_{i})\}]}\\ &= K^{-1}\log \int {E_{\textbf{y,s}\mid q}e^{h.a(\textbf{x,y,s})}\prod_{i=1}^{n}\{p(\textbf{y} \mid \textbf{x}_{i},\textbf{s})\mu_{K}(q)dq}
    \\ &= K^{-1}\log{\int{e^{Kg(q)}\mu_{K}(q)dq}}\\
    & \to \sup_{q}[g(q) - \mathscr{I}(q)], K \to \infty
\end{split}
}

\begin{shaded*}
\begin{thm}
In mathematics, \textbf{Varadhan's lemma} is a result from large deviations theory named after \textit{S. R. Srinivasa Varadhan}. The result gives information on the asymptotic distribution of a statistic $\phi(Z_\epsilon)$ of a family of random variables $Z_\epsilon$ as $\epsilon$ becomes small in terms of a rate function for the variables. 

Let X be a regular topological space; let $(Z_\epsilon)_{\epsilon>0}$ be a family of random variables taking values in $X$; let $\mu_\epsilon$ be the law (probability measure) of $Z_\epsilon$. Suppose that $(\mu_\epsilon)_{\epsilon>0}$ satisfies the large deviation principle with good rate function $\mathscr{I} : X \to [0, +\infty]$. Let $\phi  : X \to  \mathbb{R}$ be any continuous function. Suppose that at least one of the following two conditions holds true: either the tail condition

\EQ{
\lim_{M \to \infty}\lim\sup_{\epsilon \to 0} (\epsilon \log{\mathbb{E}[\exp[\frac{\phi(Z_\epsilon)}{\epsilon}]\mathbb{1}(\phi(Z_\epsilon)\geq M)]}) = -\infty
}


where $\mathbb{1}(E)$ denotes the indicator function of the event $E$; or, for some $\gamma > 1$, the moment condition
\EQ{
\lim\sup_{\epsilon \to 0}(\epsilon \log{\mathbb{E}[\exp[\gamma\frac{\phi(Z_\epsilon)}{\epsilon}]]}) < \infty
}

Then

\EQ{
\lim_{\epsilon \to 0} \epsilon \log{\mathbb{E}[\exp[\frac{\phi(Z_\epsilon)}{\epsilon}]]} = \sup_{x \in X}(\phi(x) - \mathscr{I}(x))
}
\end{thm}
\end{shaded*}

Now we assume that $\textbf{x}$ has K components, and that $p(\textbf{x})$ implies that the components $x_k$ of $\textbf{x}$ are i.i.d. We furthermore assume that $q$ consists of empirical means of $K$ i.i.d. random variables, each of which depends on $\{x_{ik} \mid i = 1, ..... ,n\}$. Then, we can apply Cramér’s theorem to the measure $\mu_{K}(q)$ to prove its large deviation property, and it turns out that the rate function $\mathscr{I}(q)$ is given in terms of the Fenchel–Legendre transform as

\EQ{
\mathscr{I}(q) = \sup_{\tilde{q}}[\tilde{q}.q - \phi(\tilde{q})]
}

where
\EQ{
\phi(\tilde{q}) \equiv \log{\sum_{\textbf{x}_{1}, ...., \textbf{x}_{n}}}e^{\tilde{q}.q\{\textbf{x}_i\}}\prod_{i =1}^{n} p({\textbf{x}_{i}})
}
is the cumulant generating function of $q\{\textbf{x}_i\}$ with respect to the prior distribution of $\{\textbf{x}_i\}$.

Now,
\EQ{
\lim_{K \to \infty} K^{-1}\log{E_{\textbf{y,s}}[\{Z(h;\textbf{y,s})\}^n]} = \sup_{q}\inf_{\tilde{q}}[g(q) - \tilde{q}.q - \phi(\tilde{q})]
}
Taking derivative with respect to and taking the limit we obtain the free energy .

\section{Replica Analysis for MPM Detectors}
In order to evaluate the free energy, we make use of the replica
method, by which we have
\EQ{
\mathscr{F} = \lim_{K \to \infty} \Bigg(\lim_{n \to 0} \frac{\partial}{\partial n}K^{-1}\log{\Xi_n}\Bigg)
}
where

\EQ{
\Xi_n = C^{-1}\int_{\mathbb{R}^N}{\overline{Z_0(\textbf{r},S)[Z(\textbf{r},S)]^n}d\textbf{r}}
}

It should be noted that, for finite $K$ and $\Xi_n$, is well-defined for real $n$. It is straightforward to see that we can exchange the order of the averaging and the differentiation with respect to $n$.In the replica method, however, we will evaluate $\Xi_n$ only for positive integers $n$.

\textit{Assumption 2}:$\Xi_n$ for real $n$ is given, at least in the vicinity of $n = 0$, by plugging the value of $n$ into the expression of $\Xi_n$ obtained by evaluating it only for positive integers $n$.

Since $[Z(\textbf{r},S)]^n$ for integer $n$ is nothing but the normalizing coefficient for a system consisting of $n$ replicas of the posterior probability sharing the same $\textbf{r}$ and $S$, we can write down $\Xi$ as

\EQ{
\Xi_n = \sum_{\textbf{x}_0,...,\textbf{x}_n}\prod_{a = 0}^{n}p(\textbf{x}_a)\Bigg\{\frac{1}{\sqrt{2\pi{\sigma_0}^2}}\int_{\mathbb{R}}{\overline{\exp\Bigg[-\frac{1}{2{\sigma_0}^2}\Bigg(r-\frac{1}{\sqrt{N}}\sum_{k=1}^{K}s_kx_{0k}\Bigg)^2\Bigg]\prod_{a=1}^{n}\exp\Bigg[-\frac{1}{\sqrt{2{\sigma}^2}}\Bigg(r - \frac{1}{\sqrt{N}}\sum_{k=1}^{K}s_kx_{ak}\Bigg)^2\Bigg]}dr}\Bigg\}^N
}

where we have introduced replicated random variables 

$$\textbf{x}_a \equiv[x_{a1}, x_{a2}, x_{a3}, .... , x_{aK}]^T \in \{-1,1\}^K, \qquad \qquad a=1,...,n$$\\
to represent the random variables of the replicated posterior probabilities. The integrand depends on $\textbf{x}$ only through

\EQ{
v_0 = \frac{1}{\sqrt{K}}\sum_{k=1}^{K}s_kx_{0k} \qquad \qquad \qquad \qquad
}

and

\EQ{
v_1 = \frac{1}{\sqrt{K}}\sum_{k=1}^{K}s_kx_{ak},\qquad \qquad a = 1, .... ,n
}

Under mild additional conditions on the statistics of $S$, these quantities can be regarded, in the $K \to \infty$ limit, as joint Gaussian random variables with means $0$ and covariances $Q_{ab} \equiv \overline{v_av_b} = K^{-1}\textbf{x}_a\textbf{x}_b$. Based on this observation, we decompose the summation over $\{\textbf{x}_a\}$ into two steps. First we perform the summation over a single subshell of the form

$$S\{Q\} \equiv \{\textbf{x}_0, . . . , \textbf{x}_n \mid \textbf{x}_a\textbf{x}_b = KQ_ab\}$$

and then integrate the result over all subshells. Thus, we have

\EQ{
\Xi_n = \int_{[-1,1]^{\frac{n(n+1)}{2}}}{\exp[K\beta^{-1}\mathscr{G}\{Q\}]\mu_K\{Q\}\prod_{a<b}dQ_{ab}}
}

where

\EQ{
\mu_K\{Q\} = \sum_{\textbf{x}_0,...,\textbf{x}_n}\prod_{a=0}^{n}p(\textbf{x}_a)\prod_{a<b}\delta(\textbf{x}_a.\textbf{x}_b - KQ_{ab})
}

is the probability weight of the subshell $S\{Q\}$, and

\EQ{
e^{\mathscr{G}(Q)} = \frac{1}{\sqrt{2\pi{\sigma_0}^2}}\int_\mathbb{R}{\overline{\exp\Bigg[-\frac{\beta}{2{\sigma_0}^2}\Bigg(\frac{r}{\sqrt{\beta}}- v_0(Q)\Bigg)^2\Bigg]\prod_{a=1}^{n}\exp\Bigg[-\frac{\beta}{2{\sigma}^2}\Bigg(\frac{r}{\sqrt{\beta}}-v_a(Q)\Bigg)^2\Bigg]}dr}
}

\textit{Assumption 3}: The order of the two limits $K \to \infty$ and $n \to 0$ can be interchanged without affecting the final result.
Based on the assumption, we have
$$\mathscr{F} = \lim_{n \to 0}\frac{\partial}{\partial n}\Bigg(\lim_{K \to \infty}K^{-1}log \Xi_n\Bigg)$$

From the theory of large deviations we know from Cramér’s theorem that the probability measure $\mu_K\{Q\}$ of the empirical means 

$$Q_{ab} = K^{-1} \sum_{k=1}^{K}x_{ak}x_{bk}$$

satisfies, as $K \to \infty$, the large deviation property with a rate $\mathscr{I}(Q)$. Then, applying Varadhan’s theorem yields
\EQ{
\lim_{K \to \infty}K^{-1}log \Xi_n = \sup_{Q}(\beta^{-1}\mathscr{G}(Q) - \mathscr{I}(Q))
}

Taking the derivative with respect to and then the limit we will obtain the final result.

\newpage

\begin{thebibliography}{9}
\bibitem{latexcompanion}
Toshiyuki Tanaka, Member IEEE.\textit{A statistical-Mechanics Approach to Large-System Analysis of CDMA Multiuser Detectors}
vol 48, pp 2888-2910, 2002

\bibitem{latexcompanion}
Dongning Guo, Member IEEE.\textit{Randomly spread CDMA: Asymptotics Via Statistical Physics}
vol 51, pp 1983-2010, 2005

\bibitem{knuthwebsite}
Boltzmann Distribution
\\\texttt{http://theoreticalminimum.com/courses/statistical-mechanics/2013/spring/lecture-4}

\end{thebibliography}

\end{document}
